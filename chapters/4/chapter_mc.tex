\startchapter{Modelling the Dark Higgs Model and Standard Model Background Processes}
\label{chapter:mc}

To search for evidence of new physics in the ATLAS data, it is necessary to develop an accurate model of the expected yield of both SM events (a.k.a. ``background") and hypothesized BSM events (a.k.a. ``signal") in the data, as well as their kinematic distributions. One can then compare the yield and kinematic distributions of events in the data with those in the signal and background model to check for any ``above-background" significant excesses in the data that could point to the presence of BSM physics. If no significant excesses are observed, the search can exclude the signal model for the range of signal model parameters for which the model predicts a significant above-background excess in the data.

Like many collider experiments, the ATLAS collaboration uses a method called ``Monte Carlo" (MC) to model the expected yield and kinematic distributions of SM and hypothesized BSM events in the data collected by the detector. The MC method is a computational algorithm which uses repeated sampling of random variables, where each set of randomly sampled variables represents a randomly generated ``event". For each event, the random variables associated with the event are passed into the model to produce a resulting set of output variables. For physical models, one is particularly interested in the set of ``observable" output variables, meaning those which can be measured experimentally in the system that is being modelled. The method is useful for complex models with many free parameters, and for which it would be unfeasible to develop analytical formulations of the distributions of observables predicted by the model. 

Given a set of MC simulated events, the associated set of values for each observable generated by passing the events through the model represents a random sampling of the underlying probability density distribution for that observable according to the model. The events can be binned into histograms in one or more observables. Assuming that, for each event, the sampling of random variables is performed ``independently" - i.e. in a manner such that the sampling of random variables for each event is unaffected by that of any other event - the number $N_i$ of events in each bin $i$ will vary randomly according to Poisson statistics with, on average, a standard deviation of $\sigma{N_i}=\sqrt{N_i}$. Consequently, as the number of MC simulated events is increased by a factor of $\alpha$, the relative size $\frac{\sigma{N_i}}{N_i}$ of fluctuations in each bin will, on average, decrease according to $\frac{1}{\sqrt{\alpha}}$. As a result, as one increases the number of MC simulated events, the shapes of histograms binned in the model's observables for the simulated events will become an increasingly precise approximation of the underlying probability distributions for these observables according to the model. 

Signal and SM background models used to perform searches for BSM physics with the ATLAS detector are produced using sophisticated MC simulations of both the passage of the final-state particles through the ATLAS detector and of the physical production mechanisms for the particle collision, production and decay processes involved. For a given process, for example the dominant \wjets background in this DM search shown in Figure zzz \textcolor{red}{(Note to Bob: will present dominant SM backgrounds, either in chapter 1 or 2)}, ``truth-level" information for each MC event is first obtained from a random proton-proton collision by simulating the physical production mechanism for the process. The set of simulated final state particles, along with their kinematic information, are collectively known as the ``truth-level" event. Truth-level events can subsequently be passed through a highly detailed  simulation of the ATLAS detector \cite{atlas_sim} produced using the Geant4 toolkit \cite{Geant4}, which models how these events would actually be measured by the detector at ``reconstruction-level". ATLAS requires very large MC data sets (millions of simulated events per process) to adequately model the predicted probability distributions for kinematic observables over their full range of interest for the measurements of SM and BSM searches that use the ATLAS data.

\section{Weighting and Normalization of MC Simulated Processes}

The process described above of 

\subsection{Weighting of MC Simulated Events}

When binning MC generated ATLAS collision events into histograms for comparison with data, 

arious multiplicative factors are applied to each event t

\begin{itemize}
\item Present the concept of cross section and luminosity, and discuss the need to scale MC simulated data to the integrated LHC data luminosity. 
\item Broadly describe how the collected data can be compared with MC simulated signal and SM background processes to search for new physics.
\end{itemize}

\section{Simulation of the DH Signal Model}

Presentation of signal grid and simulation details (eg. leading Feynman diagrams, use of FullSim, inclusion of additional final-state jet, etc.).

\section{Simulation of SM Background Processes}

Description of the background processes considered and why they make it into our analysis regions. Very brief description of how each background process is modelled (presumably don't need to get quite as in-depth as the description in section 6 of the support note). 

\subsection{\wjets and \zjets}

\subsection{\ttbar}

\subsection{Diboson}

\subsection{Triboson}

\subsection{single-top}

