\startchapter{Modelling the Dark Higgs Model and Standard Model Background Processes}
\label{chapter:mc}

To search for evidence of new physics in the ATLAS data, it is necessary to develop an accurate model of the expected yield of both SM events (a.k.a. ``background") and hypothesized BSM events (a.k.a. ``signal") in the data, as well as their kinematic distributions. One can then compare the yield and kinematic distributions of events in the data with those in the signal and background model to check for any ``above-background" significant excesses in the data that could point to the presence of BSM physics. If no significant excesses are observed, the search can exclude the signal model for the range of signal model parameters for which the model predicts a significant above-background excess in the data.

Like many collider experiments, the ATLAS collaboration uses a method called ``Monte Carlo" (MC) to model the expected yield and kinematic distributions of SM and hypothesized BSM events in the data collected by the detector. The MC method is a computational algorithm which uses repeated sampling of random variables, where each set of randomly sampled variables represents a randomly generated ``event". For each event, the random variables associated with the event are passed into the model to produce a resulting set of output variables. For physical models, one is particularly interested in the set of ``observable" output variables, meaning those which can be measured experimentally in the system that is being modelled. The method is useful for complex models with many free parameters, and for which it would be unfeasible to develop analytical formulations of the distributions of observables predicted by the model. 

Given a set of MC simulated events, the associated set of values for each observable generated by passing the events through the model represents a random sampling of the underlying probability density distribution for that observable according to the model. The events can be binned into histograms in one or more observables. Assuming that, for each event, the sampling of random variables is performed ``independently" - i.e. in a manner such that the sampling of random variables for each event is unaffected by that of any other event - the number $N_i$ of events in each bin $i$ will vary randomly according to Poisson statistics with, on average, a standard deviation of $\sigma_{N_i}=\sqrt{N_i}$. Consequently, as the number of MC simulated events is increased by a factor of $\alpha$, the relative size $\frac{\sigma_{N_i}}{N_i}$ of fluctuations in each bin will, on average, decrease according to $\frac{1}{\sqrt{\alpha}}$. As a result, as one increases the number of MC simulated events, the shapes of histograms binned in the model's observables for the simulated events will become an increasingly precise approximation of the underlying probability distributions for these observables according to the model. 

Signal and SM background models used to perform searches for BSM physics with the ATLAS detector are produced using sophisticated MC simulations of both the passage of the final-state particles through the ATLAS detector and of the physical production mechanisms for the particle collision, production and decay processes involved. For a given process, for example the dominant \wjets background in this DM search shown in Figure zzz \textcolor{red}{(Note to Bob: will present dominant SM backgrounds, either in chapter 1 or 2)}, ``truth-level" information for each MC event is first obtained from a random proton-proton collision by simulating the physical production mechanism for the process. The set of simulated final state particles, along with their kinematic information, are collectively known as the ``truth-level" event. Truth-level events can subsequently be passed through a highly detailed  simulation of the ATLAS detector \cite{atlas_sim} produced using the Geant4 toolkit \cite{Geant4}, which models how these events would actually be measured by the detector at ``reconstruction-level". ATLAS requires very large MC data sets (millions of simulated events per process) to adequately model the predicted probability distributions for kinematic observables over their full range of interest for the measurements of SM and BSM searches that use the ATLAS data.

ATLAS uses various MC simulation packages (also known as ``generators") to perform truth-level MC simulation of different physics processes. For many processes, particularly SM background processes, independent MC simulations have been performed using several different packages, and the yields and distributions of events predicted by the different packages can be compared to evaluate a systematic uncertainty associated with the choice of generator used to simulated the process. The specific generators used to model the physics processes considered in this search will be discussed in Sections \ref{sec:DH_model_sim} and \ref{sec:SM_bkg_sim}.

\section{Weighting and Normalization of MC Simulated Processes}

The process described above of 

\subsection{Weighting of MC Simulated Events}

When at various stages of the MC simulation procedure, multiplicative weight factors - or simply ``weights" - are calculated and saved for each event. These weights modify the amount by which a given event contributes to the amplitude of the bin that it gets assigned to relative to other events when the MC simulated events are binned into histograms. For a given process, the overall factor which is applied to weight each event relative to other events generated for the same process is given by the product of ``event-level weights", which are designed to apply corrections arising from various aspects of the event generation and reconstruction. 

For each event $i$, the total event-level weight is given by the following product:

\begin{equation}
\label{eq:evt_wt}
\text{event-level weight }i = \text{(generator weight)}_i \times \text{(pileup reweighting weight)}_i \times \prod_j (reconstruction weight j)
\end{equation}

The ``generator weight" is a weight applied by some generators during the generation of truth-level events for various purposes. These purposes include correcting for the generation of duplicate events at different stages of the calculation, correcting leading-order calculations to achieve the expected distributions that a more precise ``next-to-leading-order" calculation would be expected to produce. 

The ``pileup reweighting weight" is designed to account for the effects of ``pileup" \cite{pileup}, where pileup constitutes the soft QCD collision events that take place in the same (or closely-surrounding) bunch crossings as the ``hard interaction" that actually triggered the event readout. The nominal procedure of simulating the ``hard interactions" which would produce the process being modelled does not account for the presence of these pileup interactions that would be measured by the detector as part of the readout for the hard interaction of interest. Instead, a dedicated weighting factor, the so-called pileup reweighting weigh, is applied to each simulated event such that the overall normalization and shapes of observables in the MC data set properly reflects the actual pileup conditions present over the full Run 2 data-taking period.

The ``reconstruction weights" collectively refer to weights assigned to apply corrections associated with the reconstruction of objects such as electrons, muons and jets that would be produced when simulating the passage of the event through the ATLAS detector.

\subsection{Normalization of Distributions for Comparison with Data}

\begin{itemize}
\item Present the concept of cross section and luminosity, and discuss the need to scale MC simulated data to the integrated LHC data luminosity. 
\item Broadly describe how the collected data can be compared with MC simulated signal and SM background processes to search for new physics.
\end{itemize}

\section{Simulation of the DH Signal Model}
\label{sec:DH_model_sim}

Presentation of signal grid and simulation details (eg. leading Feynman diagrams, use of FullSim, inclusion of additional final-state jet, etc.).

\section{Simulation of SM Background Processes}
\label{sec:SM_bkg_sim}

Description of the background processes considered and why they make it into our analysis regions. Very brief description of how each background process is modelled (presumably don't need to get quite as in-depth as the description in section 6 of the support note). 

\subsection{\wjets and \zjets}

\subsection{\ttbar}

\subsection{Diboson}

\subsection{Triboson}

\subsection{single-top}

